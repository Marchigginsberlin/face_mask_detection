{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f695eb",
   "metadata": {},
   "source": [
    "## Basic Pipeline for image processing of our CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f386928",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39325e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26c462dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.layers import Dense, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "from tensorflow.math import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "778a0139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae879d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0572186e",
   "metadata": {},
   "source": [
    "### Import for large model with csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac82e35b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Create Dataframe of Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a2e822",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "directory_no_mask = '../raw_data/Dataset/without_mask/'\n",
    "\n",
    "no_mask_names = os.listdir(directory_no_mask)\n",
    "\n",
    "no_mask_df = pd.DataFrame()\n",
    "no_mask_df['file_name'] = no_mask_names\n",
    "no_mask_df['file_name'] = directory_no_mask + no_mask_df['file_name']\n",
    "no_mask_df['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1600b66e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "no_mask_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8da916",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "directory_wrong_mask = '../raw_data/Dataset/mask_weared_incorrect/'\n",
    "\n",
    "wrong_mask_names = os.listdir(directory_wrong_mask)\n",
    "\n",
    "wrong_mask_df = pd.DataFrame()\n",
    "wrong_mask_df['file_name'] = wrong_mask_names\n",
    "wrong_mask_df['file_name'] = directory_wrong_mask + wrong_mask_df['file_name']\n",
    "wrong_mask_df['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0283f2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wrong_mask_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fed20ca",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "directory_with_mask = '../raw_data/Dataset/with_mask/'\n",
    "\n",
    "with_mask_names = os.listdir(directory_with_mask)\n",
    "\n",
    "with_mask_df = pd.DataFrame()\n",
    "with_mask_df['file_name'] = with_mask_names\n",
    "with_mask_df['file_name'] = directory_with_mask + with_mask_df['file_name']\n",
    "with_mask_df['label'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb3bc1b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with_mask_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597c305f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([no_mask_df,wrong_mask_df,with_mask_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ea814",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af8081",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33747884",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Create Train, Test, Val Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f77256",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,0], df.iloc[:,1:], test_size=0.20, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15663b4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def read_img(image_file, label):\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.image.decode_image(image, channels = 3, dtype = tf.float32)\n",
    "    image = tf.image.resize_with_pad(image, 224, 224)\n",
    "     #image = image / 255.\n",
    "    return image, label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3444311c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_cat_train = to_categorical(y_train, num_classes=3)\n",
    "y_cat_val = to_categorical(y_val, num_classes=3)\n",
    "y_cat_test = to_categorical(y_test, num_classes=3)\n",
    "y_cat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eec82b6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.from_tensor_slices((X_train.values , y_cat_train))\n",
    "ds_train = ds_train.map(read_img).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2310023",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds_val = tf.data.Dataset.from_tensor_slices((X_val.values, y_cat_val))\n",
    "ds_val = ds_val.map(read_img).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6f6814",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds_test = tf.data.Dataset.from_tensor_slices((X_test.values, y_cat_test))\n",
    "ds_test = ds_test.map(read_img).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5305999",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Agumentation for Images (Just in Case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b303c82f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Scale the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b4749",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# not necessary with the current image import\n",
    "#X.scaled = X / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640a6a7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Built a basic CNN just like in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710adb52",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(6, kernel_size=(3, 3), activation='relu', padding='valid', input_shape=(224, 224, 3)))\n",
    "model.add(layers.Conv2D(4, kernel_size=(3), padding='valid', activation='relu')) # kernel_size = 3 <==> (3, 3)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(3, activation='softmax')) \n",
    "\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1636c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e894ede5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d5a7b9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "history =model.fit(ds_train, epochs = 10, validation_data = ds_val, callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dab20f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(ds_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f9d41a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_path = ('../models')\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5213a3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_path = ('../models')\n",
    "model = tf.keras.models.load_model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f72d340",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6e6ae7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Transfer learning with a suitable network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e534507",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcda3a34",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa10b19a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_model_1 = MobileNetV2(weights=\"imagenet\", include_top=False,input_shape = (224,224,3)) \n",
    "base_model_1.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710095dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pooling = layers.MaxPool2D(pool_size=(2,2))\n",
    "flatten_layer = layers.Flatten()\n",
    "dense_1 = layers.Dense(128, activation=\"relu\")\n",
    "drop_1 = layers.Dropout(0.3)\n",
    "prediction_1 = layers.Dense(3, activation=\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7bbb0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model1 = models.Sequential([\n",
    "    base_model_1,\n",
    "    pooling, \n",
    "    flatten_layer,\n",
    "    dense_1,\n",
    "    drop_1,\n",
    "    prediction_1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ee40d5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983080df",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab09f2f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6553dbed",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "history_1 = model1.fit(ds_train, epochs = 10, validation_data = ds_val, callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c35f9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model1.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06bbf50",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_path = ('../models/Model_1')\n",
    "model1.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adca22f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_path = ('../models/model_1.1')\n",
    "model1 = tf.keras.models.load_model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6454ccf2",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "# list all data in history\n",
    "# summarize history for accuracy\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.plot(history_1.history['accuracy'])\n",
    "plt.plot(history_1.history['val_accuracy'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "# summarize history for loss\n",
    "plt.plot(history_1.history['loss'])\n",
    "plt.plot(history_1.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba5658f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prediction_proba_m1 = model1.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0a7474",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prediction_m1 = prediction_proba_m1.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474f146b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, prediction_m1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c598ed82",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, prediction_m1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d08b17",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### NASNetMobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ed66f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import NASNetMobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49e2423",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_model_2 = NASNetMobile(\n",
    "    input_shape= None,\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    classes=1000,)\n",
    "base_model_2.trainable = False\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c285c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dense_layer = layers.Dense(500, activation = 'relu')\n",
    "prediction_layer = layers.Dense(3, activation = 'softmax')\n",
    "\n",
    "model2 = models.Sequential([\n",
    "    base_model_2, \n",
    "    prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e41affb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4901ef1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3d203",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model2.fit(ds_train, epochs = 10, validation_data = ds_val, callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8938cb7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model2.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62966015",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[0.5263108015060425, 0.5971062779426575]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8066f4c2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prediction_m2 = model2.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87713abe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, prediction_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911e028d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb59db",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2721b25",
   "metadata": {},
   "source": [
    "### Face recognition MTCNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e3f08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import mtcnn\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28470738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_picture(path_to_file):\n",
    "    img = tf.io.read_file(path_to_file)\n",
    "    img = tf.image.decode_image(img, channels = 3, dtype = tf.float32)\n",
    "    img = img.numpy()\n",
    "    return img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee2a6231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-10 11:03:19.064934: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "detector = mtcnn.MTCNN(steps_threshold=[0.7, 0.7, 0.7])\n",
    "def detect_faces(img):\n",
    "    faces= detector.detect_faces(img*255)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58851316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_from_coordinates(img,x,y,w,h):\n",
    "    return img[y:y+h,x:x+w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7e3c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_cropping(faces, img):\n",
    "    encoded_faces = []\n",
    "    for index, face in enumerate(faces):\n",
    "        encoded_faces.append(face_from_coordinates(img, *face['box']))\n",
    "    return encoded_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b56efa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_cropping(faces, img):\n",
    "    encoded_faces = []\n",
    "    for face in faces:\n",
    "        encoded_faces.append(face_from_coordinates(img, *face['box']))\n",
    "    return encoded_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a33a0c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_prediction_multi(encoded_face):\n",
    "    img = tf.image.resize_with_pad(encoded_face, 224, 224)\n",
    "    img= tf.expand_dims(img,0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "405379ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_prediction_one(encoded_face):\n",
    "    img = tf.image.resize_with_pad(encoded_face, 224, 224)\n",
    "    #img= tf.expand_dims(img,0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c4fbe",
   "metadata": {},
   "source": [
    "Funktion for face probability prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de3419cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_mask(faces, img):\n",
    "    cropped_face= face_cropping(faces, img)\n",
    "    if len(cropped_face) == 1: \n",
    "        face = prep_prediction_one(cropped_face)\n",
    "        return [model.predict(face).argmax()]\n",
    "    elif len(cropped_face) > 1: \n",
    "        faces = []\n",
    "        for j in range(len(cropped_face)):\n",
    "            face = prep_prediction_multi(cropped_face[j])\n",
    "            faces.append(face)\n",
    "        predictions = []\n",
    "        for face in faces:\n",
    "            predictions.append(model.predict(face).argmax())\n",
    "        return predictions\n",
    "    else:\n",
    "        return [3]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "651a4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(image, coordinates, prediction):\n",
    "    color_corresp = {\n",
    "      0: (255,0,0),\n",
    "      1: (255,165,0),\n",
    "      2: (0,255,0)}\n",
    "    coordinates =[faces[i]['box'] for i in range(len(faces))]\n",
    "    for i in range(len(coordinates)):\n",
    "        cv2.rectangle(image,\n",
    "                      (coordinates[i][0],coordinates[i][1]),\n",
    "                      (coordinates[i][0]+coordinates[i][2],coordinates[i][1]+ coordinates[i][3]),\n",
    "                      color_corresp[prediction[i]],3)\n",
    "    plt.imshow(image)                \n",
    "                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e021b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = ('../models/Model_1')\n",
    "model = tf.keras.models.load_model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d3de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image from file\n",
    "#filename = '../raw_data/face_no_mask/no_mask_marga.jpg'\n",
    "filename = '../raw_data/face_with_mask/mask_marga_1.jpg'\n",
    "#filename = '../raw_data/archive/images/maksssksksss4.png'\n",
    "#filename = '../raw_data/Face_with_mask/Augmented_56_323581.png'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f26e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(face_cropping(faces, img)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f226da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../raw_data/face_no_mask/marc_no_mask.jpg'\n",
    "img = load_picture(filename)\n",
    "faces = detect_faces(img)\n",
    "prediction = pred_mask(faces, img)\n",
    "print(prediction)\n",
    "draw_image(img, faces, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8902a3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Create dataset of images to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18b98a01",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#creating a test set of pictures with mask and without mask\n",
    "\n",
    "path_mask = '../raw_data/Face_Mask_Dataset/Test/WithMask'\n",
    "path_no_mask = '../raw_data/Face_Mask_Dataset/Test/WithoutMask'\n",
    "\n",
    "#path_mask = '../raw_data/Face_with_mask'\n",
    "#path_no_mask = '../raw_data/Face_no_Mask'\n",
    "\n",
    "list_mask = os.listdir(path_mask)\n",
    "list_mask.sort()\n",
    "\n",
    "\n",
    "list_no_mask = os.listdir(path_no_mask)\n",
    "list_no_mask.sort()\n",
    "\n",
    "X_model_test = []\n",
    "y_model_test = []\n",
    "\n",
    "for i in range(150,400):\n",
    "        c_path = os.path.join(path_mask, list_mask[i])\n",
    "        X_model_test.append(plt.imread(c_path)[:, :, :])\n",
    "        y_model_test.append(2) \n",
    "    \n",
    "\n",
    "for i in range(150,400):\n",
    "        c_path = os.path.join(path_no_mask, list_no_mask[i])\n",
    "        X_model_test.append(plt.imread(c_path)[:, :, :])\n",
    "        y_model_test.append(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8c51931",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-10 11:03:36.389863: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x10237c8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# detecting faces in each picture -- only one face per picture is correct\n",
    "faces_list = []\n",
    "for image in X_model_test:\n",
    "    faces = detect_faces(image)\n",
    "    faces_list.append(faces)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8da73e04",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fm/h79l0z951tj_sk_5h_0mjn7h0000gn/T/ipykernel_21492/1244963916.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  faces_array = np.array(faces_list)\n",
      "/var/folders/fm/h79l0z951tj_sk_5h_0mjn7h0000gn/T/ipykernel_21492/1244963916.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  X_model_test_array = np.array(X_model_test)\n"
     ]
    }
   ],
   "source": [
    "# sort all the pictures in which no face was detected \n",
    "\n",
    "faces_array = np.array(faces_list)\n",
    "faces_array = faces_array[[el != [] for el in faces_list]]\n",
    "\n",
    "X_model_test_array = np.array(X_model_test)\n",
    "X_model_test_array = X_model_test_array[[el != [] for el in faces_list]]\n",
    "\n",
    "y_model_test_array = np.array(y_model_test)\n",
    "y_model_test_array=y_model_test_array[[el != [] for el in faces_list]]\n",
    "\n",
    "number_of_detected_faces = len([el for el in faces_list if el != [] ])\n",
    "number_of_missed_faces = len([el for el in faces_list if el == [] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fade1af",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_missed_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0504de9d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#predict wearing a mask or not \n",
    "predictions = []\n",
    "for faces , img in zip(faces_array,X_model_test_array):\n",
    "    predictions.append(pred_mask(faces, img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38a9bbb8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Show Pictures of more than one face\n",
    "for index, el in enumerate(predictions):\n",
    "    if len(el) > 1:\n",
    "        print(index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a211211c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create an np array to pass in the confusion matrix\n",
    "pred_2 = []\n",
    "for el in predictions:\n",
    "    for e in el:\n",
    "        pred_2.append(e)\n",
    "pred_2 = np.array(pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7b112bf",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[245   2]\n",
      " [  0 197]]\n"
     ]
    }
   ],
   "source": [
    "#create the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_model_test_array, pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d22bd52",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.unique(pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753a67d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.array(y_model_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0097c9",
   "metadata": {},
   "source": [
    "### Blaze faze static "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9256f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e4658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = '../raw_data/face_no_mask/marc_no_mask.jpg'\n",
    "def load_picture(path_to_file):\n",
    "    image = cv2.imread(path_to_file)\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ee5b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection= mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)detector = mtcnn.MTCNN(steps_threshold=[0.7, 0.7, 0.7])\n",
    "\n",
    "def detect_faces(image):\n",
    "    results = face_detection.process(image)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d19b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_prediction_multi(encoded_face):\n",
    "    img = tf.image.resize_with_pad(encoded_face, 224, 224)\n",
    "    img= tf.expand_dims(img,0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_prediction_one(encoded_face):\n",
    "    img = tf.image.resize_with_pad(encoded_face, 224, 224)\n",
    "    #img= tf.expand_dims(img,0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac45470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converting_results_to_coordinates(results, image_shape):\n",
    "    faces_coordinates = []\n",
    "    if results.detections is not None:\n",
    "        for detection in results.detections:\n",
    "            box = detection.location_data.relative_bounding_box\n",
    "            x1 = max(box.xmin * image_shape[1], 1)\n",
    "            y1 = max(box.ymin * image_shape[0], 1)\n",
    "            x2 = min(x1 + box.width * image_shape[1], image_shape[1])\n",
    "            y2 = min(y1 + box.height * image_shape[0], image_shape[0])\n",
    "            faces_coordinates.append([int(x1), int(y1), int(x2), int(y2)])\n",
    "            print(faces_coordinates[-1])\n",
    "    return faces_coordinates\n",
    "\n",
    "#transforming coordinates of the face into a cropped image\n",
    "def face_from_coordinates(image,x1,y1,x2,y2):\n",
    "    return image[(y1):(y2), x1:x2]\n",
    "\n",
    "def converting_faces_to_array(image, faces_coordinates):\n",
    "    encoded_faces = []\n",
    "    for face in faces_coordinates:\n",
    "        encoded_faces.append(face_from_coordinates(image, *face))\n",
    "        #save_img('/frames/',\n",
    "    return encoded_faces\n",
    "\n",
    "def pred_mask(encoded_faces):\n",
    "    if len(encoded_faces) == 1:\n",
    "        face = prep_prediction_one(encoded_faces)\n",
    "        return [model.predict(face).argmax()]\n",
    "    elif len(encoded_faces) > 1:\n",
    "        faces = []\n",
    "        for j in range(len(encoded_faces)):\n",
    "            face = prep_prediction_multi(encoded_faces[j])\n",
    "            faces.append(face)\n",
    "        predictions = []\n",
    "        for face in faces:\n",
    "            predictions.append(model.predict(face).argmax())\n",
    "        return predictions\n",
    "    else:\n",
    "        return [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d459ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_coordinates = converting_results_to_coordinates(results, image_shape)\n",
    "\n",
    "encoded_faces = converting_faces_to_array(image, faces_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c8250",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mask(encoded_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2340a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "img = tf.io.read_file('../raw_data/face_no_mask/marc_no_mask.jpg')\n",
    "img = tf.image.decode_image(img, channels = 3, dtype = tf.uint8)\n",
    "image_shape = img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5f9655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection= mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee0842",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('../raw_data/face_no_mask/marc_no_mask.jpg')\n",
    "image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "image_shape = image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = face_detection.process(np.array(img))\n",
    "#results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "results = face_detection.process(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916801c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c90b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converting_results_to_coordinates(results, image_shape):\n",
    "    faces_coordinates = []\n",
    "    if results.detections is not None:\n",
    "        for detection in results.detections:\n",
    "            box = detection.location_data.relative_bounding_box\n",
    "            x1 = max(box.xmin * image_shape[1], 1)\n",
    "            y1 = max(box.ymin * image_shape[0], 1)\n",
    "            x2 = min(x1 + box.width * image_shape[1], image_shape[1])\n",
    "            y2 = min(y1 + box.height * image_shape[0], image_shape[0])\n",
    "            faces_coordinates.append([int(x1), int(y1), int(x2), int(y2)])\n",
    "            print(faces_coordinates[-1])\n",
    "    return faces_coordinates\n",
    "\n",
    "#transforming coordinates of the face into a cropped image\n",
    "def face_from_coordinates(image,x1,y1,x2,y2):\n",
    "    return image[(y1):(y2), x1:x2]\n",
    "\n",
    "def converting_faces_to_array(image, faces_coordinates):\n",
    "    encoded_faces = []\n",
    "    for face in faces_coordinates:\n",
    "        encoded_faces.append(face_from_coordinates(image, *face))\n",
    "        #save_img('/frames/',\n",
    "    return encoded_faces\n",
    "\n",
    "def pred_mask(encoded_faces):\n",
    "    if len(encoded_faces) == 1:\n",
    "        face = prep_prediction_one(encoded_faces)\n",
    "        return [model.predict(face).argmax()]\n",
    "    elif len(encoded_faces) > 1:\n",
    "        faces = []\n",
    "        for j in range(len(encoded_faces)):\n",
    "            face = prep_prediction_multi(encoded_faces[j])\n",
    "            faces.append(face)\n",
    "        predictions = []\n",
    "        for face in faces:\n",
    "            predictions.append(model.predict(face).argmax())\n",
    "        return predictions\n",
    "    else:\n",
    "        return [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d3ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_coordinates = converting_results_to_coordinates(results, image_shape)\n",
    "\n",
    "encoded_faces = converting_faces_to_array(image, faces_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6988fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mask(encoded_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae4a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78200d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe5d3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee61b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c831a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba9d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e0235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = cv2.cvtColor(encoded_faces[0], cv2.COLOR_RGB2BGR)\n",
    "plt.imshow(encoded_faces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93653b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(encoded_faces).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = prep_prediction_multi(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b239b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(encoded_faces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a2f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mask(encoded_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf3a064",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mask(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45895fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796b2f42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
